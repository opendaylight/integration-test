
Clustering developer point of view (intended Carbon behavior).

As ODL architecture consists of "application" bundles interacting with MD-SAL core,
clustering functionality comes as an enhancement of existing single-node core operations,
each operation obtains specific behavior with respect to cluster state
(depending on cluster configuration). In order to avoid bugs related to application behavior
(e.g. in single node, there is only one writer, but in claster there may be multiple writers
leading to colision not expected by single node application developers),
MD-SAL core now also provides several services,
which make it easier for application developers to avoid common errors.

MD-SAL operations available from within JVM:
Create and commit (standalone) read transaction against a datastore.
Create and commit (standalone) write transaction against a datastore.
Create (and close) a transaction chain, the chain allows creating transactions one at a time,
the subsequent transaction is guaranteed to see consequence of the previous transaction.
Register a provider generating a Yang notification, subscribe listeners for that.
receive notifications until unsubscription or unregistration.
Subscribe a listener for data tree changes (notifications are generated by wrtites to datastore),
listen, unsubscribe, or get notified about the unsubscription (if the listener is too slow).
Register a remote procedure call (RPC) implementation (defined as yang rpc),
either global (no specific context) or local (context defined as yang instance identifier).
Call a RPC (returns a future, that block until the result is available). Unregister a RPC.
Properties:
RPC registrations (both local and global) are cluster-wide. Multiple cluster members
may host applications registering for the same RPC (and identifier).
MD-SAL prioritizes rounting to RPC implementation located on the same member as the caller is.
Notification providers (both Yang and data change) are reachable cluster-wide,
but listener may choose to subscribe only to notifications originating on the same node.
Datastore is partitioned to shards with replication and persistence according to configuration.
At each time, a shard has at most one replica in Leader state, only this replica
is allowed to officially commit data changes. Follower replicas generate
data tree change notifications upon replicating from master.
Initial data change notification is created for a listener
the first time datastore changes from empty from the listener point of view,
either because persistence (or other application) ha justs created a nonempty state,
or the listener has subscribed later and the data was created already.
Additional services:
Entity ownership service (EOS) service which makes sure an "entity"
(sidentified by strings "type" and "id") has at most one "owner"
(elected from registered "candidates"). This can be used to avoid
the multiple writer problem. EOS implementation stores its data in operational datastore,
but it uses specific shard (not affected by configuration)
guaranteed to be replicated on every member.
Thus it makes sense to talk about EOS Leader and EOS Followers.
Singleton application service is an upgrade upon EOS. It adds the ability to group several
"applications" into groups, all aplications withing group are guranteed
to be co-located on the same member (or in the process of migration to other member).
Also, the migration is allowing new member to inherit access from the previous owner,
for example transaction chains and listener subscriptions, so the risk of data loss is reduced.
Singleton relies on EOS, but to facilitate migration, every application needs two entities
with specific type/id.

Note that aside MD-SAL, cluster operation requires other service frameworks,
namely blueprint and Config Subsystem. Those frameworks are limited to single JVM,
and they effectively act only when booting up or reconfiguring applications.
ODL code evolves in direction of moving everything important to MD-SAL,
so the other frameworks are not expected to hamper cluster operation.

The notable single-JVM framework is MBeans, in conjuction with its http interface odl-jolokia.
Some internal state (for example shart status) is only available via MBeans.

Except direct Java API into MD-SAL, there are applications which can be used to interact
with ODL remotely, especially there is Netconf and Restconf northbound interface to MD-SAL.
Restconf can trigger separate reads and separate writes to config datastore.
No transaction chain support. Restconf can (un)subscribe to existing Yang notification provider
(or data tree changes), but WebSocket is needed for obtaining the notifications.
Not sure whether the subsription is member-only, cluster-wide or configurable.
Not sure what happens if MD-SAL drops the subscription.
Restconf can call any RPC, it blocks and returns the response.

As the cluster is distributed system, partial failures are possible, and the system
should offer High Availablity to reduce impact of such failures.

Typical failures are: loss of connectivity between members and crash of a member.
Conectivity loss depends on which links are affected, which packets are filtered, and how
(packets rejected or dropped, connection resumed before or after akka timeout happens).
Crash depends on whether there was shutdown, kill or power outage,
also on whether persisted data survived (or even get corrupted).

An ideal application would continue workin with two of three members without any data loss,
and after restart/rejoin the third member would resume working seamlessly
(as if only the user lost connectivity for a while).

Note that most operations known from single member ODL contain state
that does not survive failure.
Transaction chains, notification registrations and subscriprions, RPC registrations, shard states.
That means if an application does not contain specific HA logic (e.g. Singleton)
it is expected to fail in some way in a HA scenario.

Some applications may be well-tested in stable state
(enough time before/after introducing a failure), but there may be transient states
(e.g. during election for new Leader/owner) where the application does not behave correctly,
and it might be hard to reproduce this state in system tests, as it generally depends
on specifing time ordering of "concurrent" events.

Also note that many applications interact with other devices via various protocols,
Not only cluster failures but also device (connection) failures affect the intended behavior.
Some application consist of several "microservices" which may have different behavior
with respect to failures, making it hard to figure out which behavior is intended,
and the behavior may change based on small changes to microservice implementation.
